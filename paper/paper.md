# Big Data and Privacy :hand: fa18-523-74

Yeyi Ma | Email: yeyima@umail.iu.edu | Hid: fa18-523-74

### Abstract

This paper explores big data as a smart technological aggregate of database
technologies utilized at the software and hardware levels. The paper reviews the
origin of big data and its current status in the business, technology, and
governmental circles. In the academic theory, the writer reviews the
requirements of big data technology, the architecture, and the implementation of
the same. The paper also studies the benchmarks that have been developed in
order to standardize big data. It is revealed that the technology is relatively
young to employ most of these benchmarks. In respect to privacy, big data poses
a major threat to peoples entitlement to confidential data by design. As such,
it is necessary that big data is regulated.  

## Introduction
Big data refers to the study and the application of complex data sets in
application software and hardware. Big data is associated with certain qualities
of data such as variety, velocity, volume, veracity, and value. The computer and
information age has made big data even more practical and relevant in processing
information. Today, big data entails the use of computerized systems which
execute predictive analytics and user behavior analytics. These analytics
extract value from data sets. Most processed data is available in large volumes.
However, that is not a huge problem when working with big data. To computer
systems, processing a single record will take the fraction of a second.
Processing a million records will not take a million seconds. Instead, it might
take a few extra seconds. Modern computer systems utilize economies of scale in
a manner that cannot be replicated in any other domain.  

The most important characteristic of big data is that it analyses large data
sets in a manner that is intuitive. This explains why big data has become so
popular in nearly every field in the society: government, internet search,
fin-tech, business informatics, urban informatics, medicine, and travel. Big
data poses a threat to privacy since the end user does not have the centralized
infrastructure required to make it work flawlessly. There is always a
billion-dollar company out there with access to individualsâ€™ private information
for every smart device out there.   

## Academic Theory

### Requirements
One of the main reasons why big data is growing rapidly is the increase in the
popularity of devices which fall under the internet-of-things. Therefore,
gathering the data necessary for processing is relatively easy
[@fa18-523-74-Provost2013]. Over the past 4 decades, the technological ability
to store data per capita has nearly doubled. This will only continue to grow
over the next several decades.  

The greatest concern for millennials and Gen-Z is who should stay in control of
the big data initiatives. This raises the question of privacy. Trusting big
companies such as Facebook, Apple, Google, Amazon, and Microsoft is not
considered to be a credible solution. These companies own most of the big data
infrastructure in the world. Moreover, they have a monopoly over all the
internet-of-things devices such that they can essentially shut down all the
competition by making their services and devices cheaper while harvesting user
information.  

The leaders of these companies are not elected by the public and are out to
maximize profits. This is dangerous given that they already have a monopoly over
modern communication but manage to operate as private entities capable of
ideological, cultural, and religious bias. Moreover, they are not answerable to
anyone other than their investors who usually only care about the bottom line.
Big data promises the world numerous benefits over the next several decades.
However, this comes at the cost of giving up liberties enshrined in the
constitutions of most countries that have a Bill of Rights. 

### Architecture
Big data architecture is not new in the 21st century. Database management
systems were popular in the 1990s and were offered by a few big companies. A
company such as Wintercorp became famous for issuing intuitive big data
repositories in the form of reports in this era. At the time, the biggest hard
disks were only 2.5 GB. This means that the definition of the term big data
keeps evolving in accordance with Kryder's Law[@fa18-523-74-Walter2005]. The
company has installed more of these data stores over the past 10 years. Their
largest database store is more than 50 PB.  

Other companies such as LexisNexis Group have been involved in developing the
architecture which defines big data. In 2000, LexisNexis created a C++ system
for distributing data and enforcing simple querying. This dialect uses a
technology called "apply schema" in order to infer the schema of data under
query.  

Another important organization that has been utilizing and developing big data
includes CERN[@fa18-523-74-Marx2013]. This organization has been collecting big
data for decades. The data is analyzed using supercomputers. In 2004, Alphabet
Inc. published a paper called MapReduce which employs the same structure as
CERN. The paper proposed using parallel processing to enhance speed and
accuracy. In MapReduce, queries get split and distributed over
nodes[@fa18-523-74-Dean2008]. They are then processed in a parallel manner in
what is called a Map step. The Apache open source project is one of the first to
implement the MapReduce paradigm in a project called Hadoop. However, the
MapReduce paradigm had certain limitations. As such, it was necessary to develop
the Apache Spark. This new paradigm added the ability to add many operations as
opposed to a single map. 

Today, the most popular big data architecture is the MIKE 2.0. This is an open
approach to Information System Management[@fa18-523-74-Ward2013]. It
acknowledges and addresses the need to keep revising the implications of big
data. These insights were captured in a paper called "Big Data Solution
Offering". Studies highlight that using multiple layers in big data is one of
the ways of addressing the speed problems that have persisted in big data. In
manufacturing, big data architecture is implemented as 5C. This stands for
conversion, cyber, connection, configuration, and cognition. Another vital
aspect of the big data architecture is data lake. This is a technology which
makes it possible to shift focus from centralization to a shared model in
respect to the changing information system dynamics. It also makes it possible
to segregate data within a data lake in order to minimize the overhead time. 

### Implementation

The main components of big data can be summarized into three categories:
analyzing techniques, databases, and visualizations. The data analysis
techniques include A/B testing, natural language processing, and machine
learning. On the other hand, databases include any technologies employed in the
process of data storage. This covers cloud computing as well as business
intelligence technologies. Visualization includes graphs, charts, videos, and
other technologies which are used for displaying the output data. 

In some cases, it becomes necessary to represent multidimensional big data as
cubes or tensors. To do this, it is necessary to introduce an array of database
sensors for high-level query and storage support. Other technologies that are
necessary when implementing big data in an institution include subspace
learning, tensor-based algorithms, distributed filing, HPC infrastructure, cloud
infrastructure, data mining, the world wide web, and distributed databases. It
is important to note that in spite of all the improvements seen in big data
architecture, machine learning is relatively elementary. There are numerous
challenges to machine learning. While most of these challenges are technical, a
few of them are social in regards to the interaction between individuals, the
law, and profitability. 

Companies that have implemented big data utilize the latest and greatest when it
comes to computing and storage. This means that there is a significant barrier
to full utilization of big data at an individual or SME level. Companies that
have embraced big data employ direct attached storage such as solid state drives
and high-speed SATA inside parallel processing nodes for speed. It is quite rare
for a company utilizing big data to use storage area networks or network
arranged storages. These two are perceived to be slow, expensive, and difficult
to use. If there is an existing technology in the market that performs faster,
it needs to be implemented in big data. 

Big data has numerous applications in a variety of fields. By 2010, the big data
subsector was worth at least $100 billion. It was growing at the rate of 10% per
year. This was about twice the rate of growth of the software industry in the
same year. Many developed economies in the world are continuously using
data-intensive technologies. Many people in the world have access to the
internet thanks to the development in computer software and
hardware[@fa18-523-74-Wellman1997]. The effective capacity of the world to
exchange information is growing at a rate that is unprecedented. By 2014,
internet traffic reached 667 exabytes by predictions. Big data is being utilized
in international development, manufacturing, healthcare, media, education, the
internet of things, information technology, and insurance. 

### Big Data and the Web

Big data is commonly associated to the internet of things since both concepts
are based on smart data collection and manipulation. The number of people who
rely on the internet for their work, entertainment, communication, travel
planning, and education is increasing every year. The development of the
internet is responsible for driving this shift from traditional tools to digital
ones. Big data can be seen as an augmentation of the internet. This is because
it creates a link between the physical world and software. Most efficient
programs run with a connection to the internet for the purposes of
collaboration, communication, and sharing resources. With more and more links to
big data, there is a clear conduit between the world wide web and the physical
world. 

One the most important improvements that has taken place in technology over the
past several years is the improvement in user interface that is presented to
technology users. Big data seeks to fill the gap between the user and the
software that takes instructions from various devices by collecting feedback
without too much work from the user. This explains why the internet of things is
becoming the most important component of big data. The internet of things allows
physical objects to embed operating systems in order to collect data in
real-time. When such information accumulates to volumes that can significantly
alter or influence the manner in which an object works, big data components such
as DBMS take over the records and process them yielding output that can be used
adjust certain parameters of object, an apparatus, or a system. While the
internet of things addresses the user interface solely, big data goes a step
further and provides a processing platform that can then be used to develop
useful output. Big data also allows systems to consume the feedback generated
without the intervention of a human. 

## Benchmark and Privacy

Like any other viral and potentially useful technology, big data has various
benchmarks. These benchmarks are tailored to the new technology since the
existing ones have proven to be ineffective. In spite of this, the benchmarks
that have been proposed have not been robust enough: BigBench, HiBench, AMP, and
CloudSuite[@fa18-523-74-Wang2014]. Each of these benchmarks has various merits
and demerits. With all these options, the problem remains that big data is not
mature enough to be tested in a standard environment. As such, it is prudent for
an institution to benchmark their big data implementation using the usage
scenario as opposed to these tools.  

The factors that make big data efficient are the same ones that make it a major
privacy risk. Big data analytics is dumb, meaning that it does not have
preconceived notions about the subject on which data is being collected and
processed[@fa18-523-74-Rattay2014]. This means that it is capable of collecting
more accurate information than people perceive. For instance, big data can be
used in medicine to diagnose in an objective manner. It can gather data and come
up with recommendations that have been obvious but ignored due to preconceived
notions, ideology, or human expectations. When it comes to big data, using
algorithmic black boxes can be dangerous. Such implementations leave machines to
decide what to make of inferences without the possibility of human intuition.
Multinational companies that are implementing big data and using it to sell
their services have handled this problem to a great extent. They have manual
overrides to minimize the overreach of big data. However, these companies are
not public utilities since they have a profit agenda meaning that privacy is
hardly a priority.

## Conclusions
Big data is replacing the raw information age that was ushered in by the growth
of the internet as well as internet-based technologies in the late 20th and
early 21st century. Big data makes use of numerous technologies to ensure that
data is harvested in real-time and utilized in the same fashion. With the
internet getting embedded in different home and personal appliances via embedded
operating systems, big data will continue to become more popular. The
applications of the technology are likely to outdo those of the bare internet
given that user interface is one of the key areas that big data seeks to
revolutionize. The applications will be seen in business, government,
technology, services, travel, academia, and other sectors. However, big data
poses a big threat to privacy since it eliminates the regulative human factor
from the equation in an effort to offer fluid services whenever it is
implemented. It is necessary that policymakers create regulations which protect
technology consumers without killing innovation such as big data. 
